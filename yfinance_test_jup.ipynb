{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de15fa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9580c2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1731c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import mercury as mr # for widgets\n",
    "\n",
    "from math import ceil, floor\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "# from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from plotnine import *\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "# from plotly.tools import mpl_to_plotly as ggplotly\n",
    "\n",
    "\n",
    "SEED = 3234\n",
    "CROSS_FOLDS = 10\n",
    "PERFORMANCE_METRIC = \"r2\"\n",
    "PARALLELIZATION = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319307f",
   "metadata": {},
   "source": [
    "### Interactive Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d4b9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"Checkbox\",\n    \"value\": true,\n    \"label\": \"Show code\",\n    \"model_id\": \"07133296fc184b658fcd9a57149ff846\",\n    \"code_uid\": \"Checkbox.0.50.70.1-randf4cd4173\",\n    \"url_key\": \"\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07133296fc184b658fcd9a57149ff846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mercury.Checkbox"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"App\",\n    \"title\": \"yfinance Test\",\n    \"description\": \"Testing Mercury functionality with yfinance\",\n    \"show_code\": true,\n    \"show_prompt\": false,\n    \"output\": \"app\",\n    \"schedule\": \"\",\n    \"notify\": \"{}\",\n    \"continuous_update\": false,\n    \"static_notebook\": false,\n    \"show_sidebar\": true,\n    \"full_screen\": true,\n    \"allow_download\": true,\n    \"allow_share\": true,\n    \"stop_on_error\": false,\n    \"model_id\": \"mercury-app\",\n    \"code_uid\": \"App.0.50.110.2-rand6c2bf3e0\"\n}",
      "text/html": [
       "<h3>Mercury Application</h3><small>This output won't appear in the web app.</small>"
      ],
      "text/plain": [
       "mercury.App"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_code = mr.Checkbox(value=True, label=\"Show code\")\n",
    "app = mr.App(title=\"yfinance Test\",\n",
    "             description=\"Testing Mercury functionality with yfinance\",\n",
    "            show_code=show_code.value,\n",
    "            continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16b749",
   "metadata": {},
   "source": [
    "## Add Data Wrappers\n",
    "\n",
    "An important step in engineering our program is deciding the lifespan of each piece of information passed to us. It will ultimately be convenient to certain variables around, especially when they dictate the nature of the time-series we examine (e.g. time between observations). There's ultimately a few different ways we could go about accessing this data, though the easiest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eded60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper class to bundle stock data (stored in a pandas DataFrame) together with information\n",
    "    about how the data was collected (such as what the time frame is between two data points)\n",
    "\n",
    "    A StockDataWrapper class at the minimum requires a pandas DataFrame (the primary object we are \n",
    "    interested in wrapping) as well as the respective stock symbol passed as a string so that we are\n",
    "    able to easily refer to the specific stock in later formatting. \n",
    "    \n",
    "        IMPORTANT: It is expected that the lag_lengthand num_days are set whenever the stock data is\n",
    "                loaded into the data_frame object\n",
    "    \n",
    "    Certain variables must be set (either from user input or a configuration file) prior to calling\n",
    "    a handful of methods. Specifically, the lag length must be set prior to calling any functions which\n",
    "    utilize prior data (e.g. MACD, simple_moving_average, exponential_moving_average, etc.)\n",
    "    \n",
    "    \n",
    "    Attributes:\n",
    "        data_frame:    A pandas DataFrame object containing stocks data such as Open, Close,\n",
    "            Volume, High, Low, etc.\n",
    "            \n",
    "        stock_symbol:   A string representing the stock's symbol (i.e. AAPL for Apple)\n",
    "        \n",
    "        granularity:    The time interval between two data points [1m, 2m, 5m, 15m, 30m, 1h]\n",
    "        \n",
    "        num_days:    The number of days of stock data that have been pulled \n",
    "        \n",
    "        lag_length:    The number of previous data points that are availible to use for computations\n",
    "            (such as moving averages)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, data_frame, stock_symbol):\n",
    "        \"\"\"\n",
    "        Initializes the instance based on a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data_frame: a pandas DataFrame object\n",
    "            stock_symbol: a string representing the stock's symbol (i.e. AAPL for Apple)\n",
    "        Raises:\n",
    "            ValueError: if one of the arguments is not the specified data type\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(data_frame, pd.DataFrame):\n",
    "            raise ValueError(\"data_frame must be a pandas DataFrame object\")\n",
    "        if not isinstance(stock_symbol, str):\n",
    "            raise ValueError(\"stock_symbol must be a string\")\n",
    "            \n",
    "        self.data_frame = data_frame\n",
    "        self.stock_symbol = stock_symbol\n",
    "        # granularity and num_days should be set \n",
    "        self.granularity = 0\n",
    "        self.num_days = 0\n",
    "        self.lag_length = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Indicataes how to represent a StockDataWrapper object when\n",
    "        passed as a string to print\n",
    "        \n",
    "        Returns:\n",
    "            String concatenating the __str__ of the pandas DataFrame together with\n",
    "            information on the StockDataWrapper's Varaibles\n",
    "        \"\"\"\n",
    "        my_str = f'Stock Name: {self.stock_symbol}\\n'\n",
    "        \n",
    "        # Check whether several variables have been set\n",
    "        if self.granularity == 0:\n",
    "            my_str += \"Granularity: NOT SET\\n\"\n",
    "        else:\n",
    "            my_str += f\"Granularity: {self.granularity}\\n\"\n",
    "        \n",
    "        if self.num_days == 0:\n",
    "            my_str += \"Number of days of stock data: NOT SET\\n\"\n",
    "        else:\n",
    "            my_str += f\"Number of days of stock data: {self.num_days}\\n\"\n",
    "        \n",
    "        if self.lag_length == 0:\n",
    "            my_str += \"Number of lag variables: NOT SET\\n\"\n",
    "        else:\n",
    "            my_str += f\"Number of lag variables: {self.lag_length}\\n\"\n",
    "        \n",
    "        \n",
    "        # Call the underlying Pandas DataFrame object's __str__\n",
    "        my_str += \"\\n\\n\"\n",
    "        my_str += str(self.data_frame)\n",
    "\n",
    "        return my_str\n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_lag_variables(self, num_lags):\n",
    "        \"\"\"\n",
    "        Adds precisely num_lags lag variables to the underlying data frame. \n",
    "        The lag variables are simply additional columns where the entries are shifted\n",
    "        up in time index.\n",
    "        \n",
    "            WARNING: A large number of lag variables can lead to fragmentation in the \n",
    "                DataFrame\n",
    "        \n",
    "        The (column) names of the lag variables that are added are of the form:\n",
    "                            Close_L#\n",
    "        where # is how far back that particular lag variable is looking.\n",
    "\n",
    "        Args:\n",
    "            num_lags: the number of lag variables we wish to add\n",
    "        Raises:\n",
    "            ValueError: if num_lags is not an integer or is larger than the number of\n",
    "                observations or is negative\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(num_lags, int):\n",
    "            raise ValueError(\"Parameter length must be integer\")\n",
    "        if num_lags < 1 or num_lags >= self.data_frame.shape[0]:\n",
    "            raise ValueError(\"The number of lag variables must be between\" +\n",
    "                            \" 1 and the total number of observations\")\n",
    "\n",
    "        for i in range(1,num_lags + 1):\n",
    "            index_str = \"Close_L\" + str(i)\n",
    "            self.data_frame[index_str] = self.data_frame['Close'].shift(i)\n",
    "\n",
    "        # Backfill the entries to remove any NaN\n",
    "        self.data_frame = self.data_frame.bfill(axis=0)\n",
    "        \n",
    "    \n",
    "    def delete_lag_variables(self):\n",
    "        \"\"\"\n",
    "        Removes any possible lag variables from the Data_Frame. This method\n",
    "        implicitly assumes that all lag variables follow the naming convention\n",
    "        \n",
    "                        Close_L#\n",
    "                        \n",
    "        where # is how far back that particular lag variable is looking.\n",
    "        \"\"\"\n",
    "        existing_lag_names = list(self.data_frame.filter(regex='Close_L'))\n",
    "        \n",
    "        if (len(existing_lag_names) > 0):  \n",
    "            self.data_frame = self.data_frame[\n",
    "                self.data_frame.columns.drop(existing_lag_names)\n",
    "            ]\n",
    "            # Reset lag_length so that other methods know not to compute\n",
    "            # data from lag variables\n",
    "            self.lag_length = 0\n",
    "    \n",
    "    def add_simple_moving_average(self, length):\n",
    "        \"\"\"\n",
    "        Computes the simple moving average of the stock's closing price (i.e.\n",
    "        the mean of the first 'length number' of lag variables) and adds it to a\n",
    "        new column in our pandas DataFrame. The column that is added follows\n",
    "        the naming convention:\n",
    "        \n",
    "                            SMA_(length)\n",
    "                            \n",
    "        where length is the parameter passed.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the simple moving average \n",
    "                over (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(length, int):\n",
    "            raise ValueError(\"Parameter length must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding simple moving average\")\n",
    "        if length > self.lag_length:\n",
    "            raise ValueError(\"Cannot take the average of more lag varaibles than are availible\" + \n",
    "                  f\" (currently {self.lag_length})\\n\")\n",
    "\n",
    "        # Gather the column names that we wish to take the average over\n",
    "        lag_predictors = []\n",
    "        for i in range(1, length + 1):\n",
    "            lag_name = 'Close_L' + str(i)\n",
    "            lag_predictors.append(lag_name)\n",
    "\n",
    "        # Add new column to data frame\n",
    "        column_label = \"SMA_\"  + str(length)\n",
    "        self.data_frame[column_label] = (self.data_frame[lag_predictors].sum(axis = 1,\n",
    "                                                                             skipna = True) / float(length))\n",
    "\n",
    "    \n",
    "    def add_simple_moving_standard_deviation(self, length):\n",
    "        \"\"\"\n",
    "        Computes the simple moving standard deviation of the stock's closing\n",
    "        price (i.e. the standard deviation of the first 'length number' of lag variables)\n",
    "        and adds it to a new column in our pandas DataFrame. The column that is added follows\n",
    "        the naming convention:\n",
    "        \n",
    "                            SMSD_(length)\n",
    "                            \n",
    "        where length is the parameter passed.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the simple moving standard deviation \n",
    "                over (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(length, int):\n",
    "            raise ValueError(\"Parameter length must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding simple moving average\")\n",
    "        if length > self.lag_length:\n",
    "            raise ValueError(\"Cannot take the average of more lag varaibles than are availible\" + \n",
    "                  f\" (currently {self.lag_length})\\n\")\n",
    "        \n",
    "        # Gather the column names that we wish to take the average over\n",
    "        lag_predictors = []\n",
    "        for i in range(1, length + 1):\n",
    "            lag_name = 'Close_L' + str(i)\n",
    "            lag_predictors.append(lag_name)\n",
    "            \n",
    "        # Add new column to data frame\n",
    "        column_label = \"SMSD_\"  + str(length)\n",
    "        self.data_frame[column_label] = self.data_frame[lag_predictors].std(axis = 1, skipna = True)\n",
    "          \n",
    "        \n",
    "    def add_upper_bollinger(self, length):\n",
    "        \"\"\"\n",
    "        Computes the upper Bollinger band of the stock's closing price, which is just the\n",
    "        simple moving average + the simple moving standard deviation and adds it to a new\n",
    "        column in our pandas DataFrame. The column that is added follows the naming convention:\n",
    "        \n",
    "                            upper_boll_(length)\n",
    "                            \n",
    "        where length is the parameter passed.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the bollinger band over \n",
    "                (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(length, int):\n",
    "            raise ValueError(\"Parameter length must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding simple moving average\")\n",
    "        if length > self.lag_length:\n",
    "            raise ValueError(\"Cannot take the average of more lag varaibles than are availible\" + \n",
    "                  f\" (currently {self.lag_length})\\n\")\n",
    "\n",
    "        SMSD_string = \"SMSD_\" + str(length)\n",
    "        SMA_string = \"SMA_\" + str(length)\n",
    "        \n",
    "        # Ensure that both the simple moving average data and\n",
    "        # the simple moving standard deviation data are availible in \n",
    "        # DataFrame\n",
    "        if SMSD_string not in self.data_frame.columns:\n",
    "            self.add_simple_moving_standard_deviation(length)\n",
    "        if SMA_string not in self.data_frame.columns:\n",
    "            self.add_simple_moving_average(length)\n",
    "\n",
    "        # Add new column to data frame\n",
    "        upper_bollinger_str = \"upper_boll_\" + str(length)\n",
    "        self.data_frame[upper_bollinger_str] = self.data_frame[SMA_string] + self.data_frame[SMSD_string]\n",
    "        \n",
    "        \n",
    "    def add_lower_bollinger(self, length):\n",
    "        \"\"\"\n",
    "        Computes the lower Bollinger band of the stock's closing price, which is just the \n",
    "        simple moving average + the simple moving standard deviation and adds it to a new\n",
    "        column in our pandas DataFrame. The column that is added follows the naming convention:\n",
    "        \n",
    "                            lower_boll_(length)\n",
    "                            \n",
    "        where length is the parameter passed.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the bollinger band over \n",
    "                (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "    \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(length, int):\n",
    "            raise ValueError(\"Parameter length must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding simple moving average\")\n",
    "        if length > self.lag_length:\n",
    "            raise ValueError(\"Cannot take the average of more lag varaibles than are availible\" + \n",
    "                  f\" (currently {self.lag_length})\\n\")\n",
    "\n",
    "        SMSD_string = \"SMSD_\" + str(length)\n",
    "        SMA_string = \"SMA_\" + str(length)\n",
    "        \n",
    "        # Ensure that both the simple moving average data and\n",
    "        # the simple moving standard deviation data are availible in \n",
    "        # DataFrame\n",
    "        if SMSD_string not in self.data_frame.columns:\n",
    "            self.add_simple_moving_standard_deviation(length)\n",
    "        if SMA_string not in self.data_frame.columns:\n",
    "            self.add_simple_moving_average(length)\n",
    "\n",
    "        # Add new column to data frame\n",
    "        lower_bollinger_str = \"lower_boll_\" + str(length)\n",
    "        self.data_frame[lower_bollinger_str] = self.data_frame[SMA_string] - self.data_frame[SMSD_string]\n",
    "        \n",
    "        \n",
    "    def _exponential_moving_average_helper_(self, series, smoothing_factor):\n",
    "        \"\"\"\n",
    "        Helper function for add_exponential_moving_average() which utilizes a \n",
    "        temporary buffer in memory to compute the recursive function:\n",
    "        \n",
    "            EMA(data, n+1) = β * data[n+1] +  (1 - β) * data[n]\n",
    "            \n",
    "        where β represents the smoothing factor. The smoothing factor ultimately dictates\n",
    "        how heavily recent data points are weighted, and how quickly previous data points\n",
    "        lose weight\n",
    "\n",
    "        Args:\n",
    "            series: a pandas.core.series.Series object. Typically passed as\n",
    "                dataframe[\"column\"].iloc[a:b]. Must be treated slightly differently than\n",
    "                a standard array since integer indexing of series is depricated\n",
    "            \n",
    "            smoothing_factor: a floating point integer which dictates how heavily recent\n",
    "                data points are weighted\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(series, pd.Series):\n",
    "            raise ValueError(\"Parameter series must be pandas.Series data type\")\n",
    "        if not isinstance(smoothing_factor, float):\n",
    "            raise ValueError(\"Parameter smoothing factor must be a floating point decimal\")\n",
    "        \n",
    "        N = series.shape[0]\n",
    "        # Create temporary storage to compute our outputs. When the lag_length of\n",
    "        # the exponential moving average is large, this will likely lead to memory fragmentation\n",
    "        buffer = list(range(N))\n",
    "        buffer[0] = series.iloc[0]\n",
    "\n",
    "        # Use temporary storage + iteration to compute the recursive formula\n",
    "        #\n",
    "        #       EMA(data, n+1) = β * data[n+1] +  (1 - β) * data[n]\n",
    "        #\n",
    "        for i in range(1,N):\n",
    "            buffer[i] = smoothing_factor * series.iloc[i] + (1 - smoothing_factor) * buffer[i-1]\n",
    "        \n",
    "        # return the last data point (i.e. the EMA over the desired lag_length)\n",
    "        return buffer[-1]\n",
    "\n",
    "\n",
    "\n",
    "    def add_exponential_moving_average(self, length):\n",
    "        \"\"\"\n",
    "        Computes the exponential moving standard average of the stock's closing\n",
    "        price (i.e. the weighted average of the first [length] lag variables)\n",
    "        and adds it to a new column in our pandas DataFrame. The column that is added follows\n",
    "        the naming convention:\n",
    "        \n",
    "                            SMSD_(length)\n",
    "                            \n",
    "        where length is the parameter passed.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the simple moving standard deviation \n",
    "                over (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(length, int):\n",
    "            raise ValueError(\"Parameter lag_length must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding exponental moving average\")\n",
    "        if length > self.lag_length:\n",
    "            raise ValueError(\"Cannot take the average of more lag varaibles than are availible\" + \n",
    "                      f\" (currently {self.lag_length})\\n\")\n",
    "\n",
    "        N = self.data_frame.shape[0]\n",
    "        smoothing_factor = float(2/(length + 1))\n",
    "\n",
    "        # Temporary output storage\n",
    "        buffer = list(range(N))\n",
    "        buffer[0] = self.data_frame[\"Close\"].iloc[0]\n",
    "        buffer[1] = self.data_frame[\"Close\"].iloc[0]\n",
    "\n",
    "\n",
    "        for i in range(2, N):\n",
    "            # If there are less that lag_period of data previous to the current date,\n",
    "            # simply take the average of all the days prior to get the closest thing\n",
    "            # to a weighted average\n",
    "            if i <= length:\n",
    "                buffer[i] = self._exponential_moving_average_helper_(\n",
    "                    self.data_frame[\"Close\"].iloc[0:i],\n",
    "                    2/(i + 1)\n",
    "                )\n",
    "            else:\n",
    "                buffer[i] = self._exponential_moving_average_helper_(\n",
    "                    self.data_frame[\"Close\"].iloc[i-length:i],\n",
    "                    smoothing_factor\n",
    "                )\n",
    "        # Add new column to data frame\n",
    "        column_label = \"EMA_\"  + str(length)\n",
    "        self.data_frame[column_label] = buffer\n",
    "\n",
    "\n",
    "    def add_MACD(self, max_lag, min_lag):\n",
    "        \"\"\"\n",
    "        Computes the moving average convergence-divergence of the stock's closing\n",
    "        price (i.e. the difference of the exponential moving average taken over min_lag\n",
    "        with the exponential moving average taken over max_lag)\n",
    "        and adds it to a new column in our pandas DataFrame. The column that is added follows\n",
    "        the naming convention:\n",
    "        \n",
    "                            MACD_(min_lag)_(max_lag)\n",
    "                            \n",
    "        where min_lag and max_lag are the first and second parameters passed, respectively.\n",
    "\n",
    "        Args:\n",
    "            length: the number of days to take the simple moving standard deviation \n",
    "                over (equivalently, the number of lag variables we are considering)\n",
    "        Raises:\n",
    "            ValueError: if length is not an integer or is larger than the number of\n",
    "                lag variables availible\n",
    "            NameError: if lag_length has not been set yet (i.e. no lag variables have\n",
    "                been added)\n",
    "        \"\"\"\n",
    "        \n",
    "        #################\n",
    "        #  ERROR HANDLING\n",
    "        #################\n",
    "        if not isinstance(max_lag, int) or not isinstance(min_lag, int):\n",
    "            raise ValueError(\"Parameters max_lag and min_lag must be integer\")\n",
    "        if self.lag_length == 0:\n",
    "            raise NameError(\"Lag variables must be set prior to adding MACD\")\n",
    "        if min_lag >= max_lag:\n",
    "            raise ValueError(\"min_lag must be strictly smaller than max_lag\")\n",
    "\n",
    "        EMA_string_min = \"EMA_\" + str(min_lag)\n",
    "        EMA_string_max = \"EMA_\" + str(max_lag)\n",
    "\n",
    "        # Ensure that both exponential moving averages are availible\n",
    "        # in the DataFrame\n",
    "        if EMA_string_min not in self.data_frame.columns:\n",
    "            self.add_exponential_moving_average(min_lag)\n",
    "        if EMA_string_max not in self.data_frame.columns:\n",
    "            self.add_exponential_moving_average(max_lag)\n",
    "            \n",
    "        \n",
    "        # Add new column to data frame\n",
    "        column_label = \"MACD_\"  + str(min_lag) + \"_\" + str(max_lag)\n",
    "        self.data_frame[column_label] = (self.data_frame[EMA_string_min] - self.data_frame[EMA_string_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2df05",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c4c9c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# STEP 1: OBTAIN DESIRED STOCK + INTERVALS\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m##################################\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m stock_symbol \u001b[38;5;241m=\u001b[39m \u001b[43mmr\u001b[49m\u001b[38;5;241m.\u001b[39mText(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease input the stock symbol you would like to examine: (e.g. AAPL)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                        value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m num_days_to_build \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mSlider(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many days of intraday stock market data should we use to build our model?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                               value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m                               \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m                               \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     11\u001b[0m granularity_input \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mSelect(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow often should our model look at stock prices?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m                                 value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                 choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m15m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m30m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m60m\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mr' is not defined"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# STEP 1: OBTAIN DESIRED STOCK + INTERVALS\n",
    "##################################\n",
    "\n",
    "stock_symbol = mr.Text(label=\"Please input the stock symbol you would like to examine: (e.g. AAPL)\",\n",
    "                       value=\"AAPL\")\n",
    "num_days_to_build = mr.Slider(label=\"How many days of intraday stock market data should we use to build our model?\",\n",
    "                              value=1,\n",
    "                              min=1,\n",
    "                              max=7)\n",
    "granularity_input = mr.Select(label=\"How often should our model look at stock prices?\",\n",
    "                                value=\"1m\",\n",
    "                                choices=[\"1m\", \"2m\", \"5m\", \"15m\", \"30m\", \"60m\"])\n",
    "\n",
    "# Apply num_days information to create Datetime variables\n",
    "# which give a range of precisely (num_days) previous days from\n",
    "# current date.\n",
    "yesterday = date.today()\n",
    "most_recent_stock_day = yesterday\n",
    "    \n",
    "# yfinance can only see the previous day's 24-hour stock period. \n",
    "# Thus, if the day is Sunday or Monday, there will be no stock data\n",
    "# availible when probing date.today() in yfinance\n",
    "if yesterday.weekday() == 6:\n",
    "        most_recent_stock_day = yesterday - timedelta(1)\n",
    "elif yesterday.weekday() == 0:\n",
    "        most_recent_stock_day = yesterday - timedelta(2)\n",
    "    \n",
    "num_days_prior = most_recent_stock_day - timedelta(num_days_to_build.value)\n",
    "\n",
    "df = pd.DataFrame(yf.download(stock_symbol.value,\n",
    "                                start=num_days_prior,\n",
    "                                end=most_recent_stock_day,\n",
    "                                interval=granularity_input.value)\n",
    "                            )\n",
    "    \n",
    "# add user input information to StockDataWrapper object\n",
    "stonks = StockDataWrapper(df, stock_symbol.value)\n",
    "stonks.num_days = num_days_to_build.value\n",
    "stonks.granularity = granularity_input.value\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fig = px.line(stonks.data_frame, x=stonks.data_frame.index,\n",
    "                  y='Close',\n",
    "                 title=f\"{stonks.stock_symbol} Stock Prices\")\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "##################################\n",
    "# STEP 2: SET UP TIME-SERIES DATA\n",
    "##################################\n",
    "\n",
    "lag_length = mr.Slider(label=f\"How many previous {stonks.granularity}in intervals should our machine learning models look at?\",\n",
    "                       value=15,\n",
    "                       min=1,\n",
    "                       max=int(0.3 * stonks.data_frame.shape[0])\n",
    "                      )\n",
    "stonks.lag_length = lag_length.value\n",
    "stonks.add_lag_variables(stonks.lag_length)\n",
    "\n",
    "SMA_string = 'SMA_' + str(stonks.lag_length)\n",
    "lower_boll_str = 'lower_boll_' + str(stonks.lag_length)\n",
    "upper_boll_str = 'upper_boll_' + str(stonks.lag_length)\n",
    "EMA_string = 'EMA_' + str(stonks.lag_length)\n",
    "EMA_string_2 = 'EMA_' + str(int(stonks.lag_length / 2))\n",
    "MACD_string = 'MACD_' + str(int(stonks.lag_length / 2)) + '_' + str(stonks.lag_length)\n",
    "\n",
    "stonks.add_simple_moving_average(stonks.lag_length)\n",
    "stonks.add_upper_bollinger(stonks.lag_length)\n",
    "stonks.add_lower_bollinger(stonks.lag_length)\n",
    "stonks.add_exponential_moving_average(stonks.lag_length)\n",
    "stonks.add_exponential_moving_average(int(stonks.lag_length/2))\n",
    "stonks.add_MACD(stonks.lag_length, int(stonks.lag_length/2))\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fig = px.line(stonks.data_frame, x=stonks.data_frame.index,\n",
    "                  y=[SMA_string, upper_boll_str, lower_boll_str, 'Close',], \n",
    "                  color_discrete_map={\n",
    "                      SMA_string: \"#85deb1\",\n",
    "                      upper_boll_str: \"#b6d1c3\",\n",
    "                      lower_boll_str: \"#b6d1c3\",\n",
    "                      \"Close\": \"#0059ff\",\n",
    "                  },\n",
    "                 title=f\"{stonks.stock_symbol} Stock Prices w/ Bollinger Bands\").update_traces(\n",
    "        selector={\"name\": upper_boll_str},\n",
    "        line={\"dash\": \"dot\"}\n",
    "    ).update_traces(\n",
    "        selector={\"name\": lower_boll_str}, \n",
    "        line={\"dash\": \"dot\"})\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fig = px.line(stonks.data_frame, x=stonks.data_frame.index,\n",
    "                  y=[EMA_string, EMA_string_2], \n",
    "                  color_discrete_map={\n",
    "                      EMA_string:\"#38b9ff\",\n",
    "                      EMA_string_2:\"#44fcd1\"\n",
    "                  },\n",
    "                 title=f\"{stonks.stock_symbol} Exponential Moving Averages\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fig = px.line(stonks.data_frame, x=stonks.data_frame.index,\n",
    "                  y=MACD_string, \n",
    "                  color_discrete_map={\n",
    "                      MACD_string:\"#edff47\"\n",
    "                  },\n",
    "                 title=f\"{stonks.stock_symbol} MACD\").update_layout(height=220)\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98f2d4",
   "metadata": {},
   "source": [
    "# Setting Up Models\n",
    "\n",
    "After examining the data over our fixed period, it is now time to construct several machine learning models which will ultimately be used to forecast future prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f40bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"Select\",\n    \"value\": \"Gradient-Boosted Trees\",\n    \"choices\": [\n        \"Linear Regression\",\n        \"k-Nearest Neighbors\",\n        \"Random Forest\",\n        \"Gradient-Boosted Trees\"\n    ],\n    \"label\": \"What statistical learning model would you like to use to predict stock prices?\",\n    \"model_id\": \"2b89a5da605942ec84fabe13ea152495\",\n    \"code_uid\": \"Select.0.50.104.11-rande2786aaa\",\n    \"url_key\": \"\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b89a5da605942ec84fabe13ea152495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mercury.Select"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictors\n",
    "predictors = stonks.data_frame.filter(regex='Close_L')\n",
    "# Outcome\n",
    "outcome=stonks.data_frame[\"Close\"]\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(predictors, \n",
    "                                                  outcome,\n",
    "                                                  random_state=SEED,\n",
    "                                                  test_size=0.25)\n",
    "\n",
    "model_choice = mr.Select(value = \"Gradient-Boosted Trees\",\n",
    "                         choices = [\"Linear Regression\",\n",
    "                                    \"k-Nearest Neighbors\",\n",
    "                                   \"Random Forest\",\n",
    "                                   \"Gradient-Boosted Trees\"],\n",
    "                         label = \"What statistical learning model would you like to use to predict stock prices?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6937641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice.value == \"Linear Regression\":\n",
    "    \n",
    "    mr.Md(\"## Linear Regression Models\")\n",
    "    mr.Md(\"---------\") \n",
    "    mr.Md(\"The simplest models we wish to consider in this analysis are modifications of the usual OLS (ordinary least squares) regression algorithms. \")\n",
    "    mr.Md(\"Similar to above, we provide two different means for users to tune the models: direct keyboard input, or reading from a ` .config` file.\")\n",
    "        \n",
    "    mr.Md(\"Similar to before, parsing user input data from scratch actually requires significantly more code than setting up the models themselves. With the parameters in hand, we can run\")\n",
    "    \n",
    "    ############################\n",
    "    # Step 1: Construct Pipeline\n",
    "    ############################\n",
    "\n",
    "    pipe_elastic_net = Pipeline([\n",
    "        ('scaler', preprocessing.StandardScaler()), \n",
    "        ('classifier', ElasticNet())\n",
    "    ])\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 2: Gather user input\n",
    "    ################################# \n",
    "    alpha_exponents = mr.Range(label=\"Provide an exponent range for N such that 2^N is our penalty terms\", value=[-5,3], min=-12, max=10)\n",
    "    num_l1_ratios = mr.Slider(value=10, min=1, max=20, label=\"How many variations of Lasso / Ridge regression should we consider?\", step=1)\n",
    "\n",
    "\n",
    "    alphas = [(2**alpha) for alpha in range(alpha_exponents.value[0], alpha_exponents.value[1])]\n",
    "    l1_ratios = np.linspace(start=0, stop=1, num=num_l1_ratios.value, endpoint=True)\n",
    "\n",
    "    params = {\"classifier__alpha\": alphas,\n",
    "                  \"classifier__l1_ratio\": l1_ratios}  \n",
    "\n",
    "    #################################\n",
    "    # Step 3: Grid Search\n",
    "    ################################# \n",
    "\n",
    "    # NOTE: When parallelization is applied, the parallel tasks do NOT\n",
    "    # have warnings supressed\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        mr.Md(f\"\\n\\nRunning {CROSS_FOLDS * len(alphas) * len(l1_ratios)} models .....\\n\\n\" )\n",
    "        search = GridSearchCV(pipe_elastic_net,\n",
    "                              params,\n",
    "                              scoring=PERFORMANCE_METRIC,\n",
    "                              cv=CROSS_FOLDS)\n",
    "        search.fit(X_train, Y_train)\n",
    "\n",
    "    # predY = grid.predict(testX)\n",
    "\n",
    "    #################################\n",
    "    # Step 4: Plot Grid Performance\n",
    "    #################################\n",
    "    scores_mean = search.cv_results_['mean_test_score'].reshape(len(alphas),\n",
    "                                                              len(l1_ratios))\n",
    "\n",
    "\n",
    "    elastic_net_performance = pd.DataFrame({\n",
    "        \"L1_Ratio\" : l1_ratios\n",
    "    })\n",
    "\n",
    "    for idx, val in enumerate(alphas):\n",
    "        alpha_str = \"alpha_\" + str(val)\n",
    "        elastic_net_performance[alpha_str] = scores_mean[idx,:]\n",
    "\n",
    "    fig = px.line(elastic_net_performance, x=\"L1_Ratio\",\n",
    "                      y=[col for col in elastic_net_performance.columns if 'alpha' in col],\n",
    "                     title=\"Elastic Net Performance\").update_layout(\n",
    "        yaxis_title=\"R^2 value\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    mr.Md(f\"The best performing Elastic Net regression model had an L1 ratio of {search.best_params_['classifier__l1_ratio']}\" + \n",
    "         f\"\\n(i.e. {100*search.best_params_['classifier__l1_ratio']}% Lasso regression)\\n\" +\n",
    "         f\"together with penalty coefficient of {search.best_params_['classifier__alpha']}\" +\n",
    "         \"\\nThis gave a mean R^2 score of \" +\n",
    "         f\"{search.best_score_} across the {CROSS_FOLDS} folds of the training data.\\n\")\n",
    "\n",
    "\n",
    "    mr.Md(\"\\n\\n\")\n",
    "    mr.Md(f'Training set score: {str(search.score(X_train, Y_train))}')\n",
    "    mr.Md(f'Test set score: {str(search.score(X_test, Y_test))}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "906ab238",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice.value == \"k-Nearest Neighbors\": \n",
    "    \n",
    "    mr.Md(\"## $k$-Nearest Neighbors\")\n",
    "    mr.Md(\"----------\")\n",
    "    mr.Md(\"A potential downside to regression based models is that they are highly mechanistic: we make a huge jump in assuming that the relationship between the predictors (i.e. previous day's prices) and the outcome (current price) follows a linear trend. By instead looking at empirically-driven models, we do not make any underlying assumptions about the relationship between our previous prices and current prices.\")\n",
    "    mr.Md('For $k$-Nearest neighbors specifically, we require some sort of notion of \"distance\" between our points')\n",
    "    mr.Md(\"However, if one wishes to run the code using a pre-determined `.config` file, the following values must be set:\")\n",
    "    mr.Md(\"```\\nclassifier__n_neighbors: min_neighbors, max_neighbors, step_size\\nclassifier__p: p_val\\n```\")\n",
    "    mr.Md(\"Upon loading the necessary user data, we proceed as before and construct a `sklearn.pipeline.Pipeline` object which we will then pass to a grid search:\")\n",
    "    \n",
    "    ############################\n",
    "    # Step 1: Construct Pipeline\n",
    "    ############################\n",
    "    pipe_knn = Pipeline([\n",
    "        ('scaler', preprocessing.StandardScaler()), \n",
    "        ('classifier', KNeighborsRegressor())\n",
    "    ])\n",
    "\n",
    "    #################################\n",
    "    # Step 2: Gather user input\n",
    "    ################################# \n",
    "\n",
    "    neighbors_input = mr.Range(label=\"Give a range for the number of nearby data points our model should look at:\", value=[1,10],\n",
    "                                   min=1,\n",
    "                                   max=min(stonks.data_frame.shape[0], 200))\n",
    "\n",
    "    p_val_input = mr.Slider(value=5, min=1, max=10, label=\"Whats the largest L^p distance we should examine?\", step=0.5)\n",
    "\n",
    "    num_neighbors = list(set(np.linspace(neighbors_input.value[0],\n",
    "                                neighbors_input.value[1],\n",
    "                                20,\n",
    "                                dtype=int,\n",
    "                                endpoint=True)))\n",
    "    p_values = np.arange(1, p_val_input.value + 0.5, 0.5)\n",
    "\n",
    "    params = {\"classifier__n_neighbors\": num_neighbors,\n",
    "               \"classifier__p\": p_values}\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 3: Grid Search\n",
    "    ################################# \n",
    "\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        print(f\"\\n\\nRunning {CROSS_FOLDS * len(p_values) * len(num_neighbors)} models .....\\n\\n\" )\n",
    "\n",
    "        search = GridSearchCV(estimator=pipe_knn,\n",
    "                              param_grid=params,\n",
    "                              scoring = PERFORMANCE_METRIC,\n",
    "                              n_jobs = PARALLELIZATION,\n",
    "                              cv = CROSS_FOLDS)\n",
    "\n",
    "        search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 4: Plot Grid Performance\n",
    "    #################################\n",
    "\n",
    "\n",
    "    scores_mean = search.cv_results_['mean_test_score'].reshape(len(num_neighbors),\n",
    "                                                              len(p_values))\n",
    "    knn_performance = pd.DataFrame({\n",
    "        \"num_neighbors\" : num_neighbors\n",
    "    })\n",
    "\n",
    "    for idx, val in enumerate(p_values):\n",
    "        p_str = \"p_\" + str(val)\n",
    "        knn_performance[p_str] = scores_mean[:,idx]\n",
    "\n",
    "    fig = px.line(knn_performance, x=\"num_neighbors\",\n",
    "                      y=[col for col in knn_performance.columns if 'p_' in col],\n",
    "                     title=\"k-Nearest Neighbors\").update_layout(\n",
    "        yaxis_title=\"R^2 value\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    mr.Md(f\"The best performing k-Nearest Neighbors model looked at {search.best_params_['classifier__n_neighbors']}\" + \n",
    "         f\" neighboring data points\\n\" +\n",
    "         f\"using the L{search.best_params_['classifier__p']} metric (i.e. the distance between points x and y \" + \n",
    "          f\" is |x^{search.best_params_['classifier__p']} - y^{search.best_params_['classifier__p']}|^(1/{search.best_params_['classifier__p']}))\" +\n",
    "         \"\\nThis gave a mean R^2 score of \" +\n",
    "         f\"{search.best_score_} across the {CROSS_FOLDS} folds of the training data.\\n\")\n",
    "\n",
    "    mr.Md(\"\\n\\n\")\n",
    "    mr.Md(f'Training set score: {str(search.score(X_train, Y_train))}' )\n",
    "    mr.Md(f'Test set score: {str(search.score(X_test, Y_test))}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39f39a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice.value == \"Random Forest\": \n",
    "    \n",
    "    mr.Md(\"## Tree-Based Algorithms\")\n",
    "    mr.Md(\"------\")\n",
    "    mr.Md(\"Similar to $k$-Nearest neighbors, tree-based algorithms are empirically-driven models which determine an \" +\n",
    "         \"outcome based off previous data points in the training set. However, tree-based models wind up being \" +\n",
    "         \"significantly more robust as they do not ultimately depend on the metric structure of the predictor space \" +\n",
    "         \"(though they do depend on the underlying topology, as they are ultimately dependent on decision-trees).\")\n",
    "    \n",
    "    mr.Md(\"<br/>\")\n",
    "    mr.Md(\"### Random Forest\")\n",
    "    mr.Md(\"--------\")\n",
    "    mr.Md(\"Some description of random forest here\")\n",
    "\n",
    "    ############################\n",
    "    # Step 1: Construct Pipeline\n",
    "    ############################\n",
    "    pipe_random_forest = Pipeline([\n",
    "            ('scaler', preprocessing.StandardScaler()), \n",
    "            ('classifier', RandomForestRegressor(random_state=SEED))\n",
    "        ])\n",
    "\n",
    "    #################################\n",
    "    # Step 2: Gather user input\n",
    "    ################################# \n",
    "\n",
    "\n",
    "\n",
    "    features_input = mr.Range(label=f\"How many random {stonks.granularity}in intervals each tree in our forest can see?\",\n",
    "                              value=[int(stonks.lag_length/4), int(stonks.lag_length/2)],\n",
    "                              min=1,\n",
    "                              max=int(stonks.lag_length))\n",
    "\n",
    "    estimators_input = mr.Range(label=f\"How many trees should be in our forest?\",\n",
    "                              value=[10, 20],\n",
    "                              min=1,\n",
    "                              max=150)\n",
    "    leaf_input = mr.Range(label=f\"How many samples should be in a tree branch before it splits?\",\n",
    "                              value=[1, 10],\n",
    "                              min=1,\n",
    "                              max=min(stonks.data_frame.shape[0], 200))\n",
    "\n",
    "    n_features = list(set(np.linspace(features_input.value[0],\n",
    "                                      features_input.value[1],\n",
    "                                      10,\n",
    "                                      dtype=int,\n",
    "                                      endpoint=True) ))\n",
    "    n_estimators = list(set(np.linspace(estimators_input.value[0],\n",
    "                                      estimators_input.value[1],\n",
    "                                      10,\n",
    "                                      dtype=int,\n",
    "                                      endpoint=True) ))\n",
    "    n_leaf = list(set(np.linspace(leaf_input.value[0],\n",
    "                                      leaf_input.value[1],\n",
    "                                      5,\n",
    "                                      dtype=int,\n",
    "                                      endpoint=True) ))\n",
    "\n",
    "\n",
    "\n",
    "    params={'classifier__max_features': n_features,\n",
    "            'classifier__n_estimators': n_estimators,\n",
    "            'classifier__min_samples_leaf':n_leaf\n",
    "            }\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 3: Grid Search\n",
    "    ################################# \n",
    "\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "        print(f\"\\n\\nRunning {CROSS_FOLDS * len(n_features) * len(n_leaf) * len(n_estimators)} models .....\\n\\n\" )\n",
    "        search = GridSearchCV(pipe_random_forest,\n",
    "                              params,\n",
    "                              scoring = PERFORMANCE_METRIC,\n",
    "                              cv=CROSS_FOLDS,\n",
    "                              n_jobs=PARALLELIZATION,\n",
    "                             )\n",
    "        search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 4: Plot Grid Performance\n",
    "    #################################\n",
    "    scores_mean = search.cv_results_['mean_test_score'].reshape(len(n_features), \n",
    "                                                                len(n_leaf),\n",
    "                                                               len(n_estimators))\n",
    "    \n",
    "    \n",
    "    # create pandas.DataFrame to pr\n",
    "    random_forest_performance = pd.DataFrame({\n",
    "        \"Trees\" : n_estimators\n",
    "    })\n",
    "\n",
    "\n",
    "    titles=[f'min_samples_leaf={leaf}' for leaf in n_leaf]\n",
    "    subs= make_subplots(rows=len(n_leaf),\n",
    "                        cols=1, \n",
    "                        subplot_titles=titles\n",
    "                       )\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for idx_1, val_1 in enumerate(n_leaf):\n",
    "            for idx_2, val_2 in enumerate(n_features):\n",
    "                feature_str = \"features_\" + str(val_2)  + \"_leaf_\" + str(val_1)\n",
    "                random_forest_performance[feature_str] = scores_mean[idx_2,idx_1,:]\n",
    "\n",
    "            figure_traces = []\n",
    "            fig = px.line(random_forest_performance, x=\"Trees\",\n",
    "                          y=[col for col in random_forest_performance.columns if f'leaf_{val_1}' in col],\n",
    "                         title=f\"Random Forest Performance (leaf={val_1})\").update_layout(yaxis_title=\"R^2 value\")\n",
    "            for trace in range(len(fig[\"data\"])):\n",
    "                figure_traces.append(fig[\"data\"][trace])\n",
    "            for traces in figure_traces:\n",
    "                subs.append_trace(traces, row=(idx_1 + 1), col=1)\n",
    "\n",
    "            subs.update_xaxes(title_text=\"# Trees\", row=(idx_1 + 1), col=1)\n",
    "\n",
    "\n",
    "        subs.update_layout(height=(220 * len(n_leaf)), title_text=\"Random Forest Performance\")\n",
    "        subs.show()        \n",
    "\n",
    "\n",
    "    mr.Md(f\"\\nThe best performing Random Forest had {search.best_params_['classifier__n_estimators']} trees;\" +\n",
    "         \"\\nin each tree, a split point is only considered if it leaves \" +\n",
    "          f\"{search.best_params_['classifier__min_samples_leaf']} samples in each of the left & right branches.\" +\n",
    "         f\"\\nEach decision tree only considered {search.best_params_['classifier__max_features']}\" +\n",
    "         f\" {stonks.granularity}in intervals when looking for the best split.\\nThis gave a mean R^2 score of \" +\n",
    "         f\"{search.best_score_} across the {CROSS_FOLDS} folds of the training data.\\n\")    \n",
    "\n",
    "\n",
    "    mr.Md(\"\\n\\n\")\n",
    "    mr.Md(f'Training set score: {str(search.score(X_train, Y_train))}' )\n",
    "    mr.Md(f'Test set score: {str(search.score(X_test, Y_test))}' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9e9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tree-Based Algorithms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Similar to $k$-Nearest neighbors, tree-based algorithms are empirically-driven models which determine an outcome based off previous data points in the training set. However, tree-based models wind up being significantly more robust as they do not ultimately depend on the metric structure of the predictor space (though they do depend on the underlying topology, as they are ultimately dependent on decision-trees)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gradient Boosted Trees"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "--------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Some description of boosted trees here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"Range\",\n    \"value\": [\n        3,\n        7\n    ],\n    \"min\": 1,\n    \"max\": 15,\n    \"step\": 1,\n    \"label\": \"How many random 1min intervals each tree in our forest can see?\",\n    \"model_id\": \"79feb1f8742e4875aaf71d328f82f48a\",\n    \"code_uid\": \"Range.0.50.90.28-rand52f13211\",\n    \"url_key\": \"\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79feb1f8742e4875aaf71d328f82f48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mercury.Range"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"Range\",\n    \"value\": [\n        10,\n        20\n    ],\n    \"min\": 1,\n    \"max\": 150,\n    \"step\": 1,\n    \"label\": \"How many trees should be in our forest?\",\n    \"model_id\": \"84e8ad3ac8af4debb2e28ea96e45a61d\",\n    \"code_uid\": \"Range.0.50.90.33-rande40fdbfe\",\n    \"url_key\": \"\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e8ad3ac8af4debb2e28ea96e45a61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mercury.Range"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/mercury+json": "{\n    \"widget\": \"Range\",\n    \"value\": [\n        -8,\n        -1\n    ],\n    \"min\": -12,\n    \"max\": 4,\n    \"step\": 1,\n    \"label\": \"Provide an exponent range for N such that 2^N is our learning rate\",\n    \"model_id\": \"c7a2d9801ffc46e3b3a747899c21f44b\",\n    \"code_uid\": \"Range.0.50.90.37-rand3ddc171d\",\n    \"url_key\": \"\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a2d9801ffc46e3b3a747899c21f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mercury.Range"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running 4000 models .....\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model_choice.value == \"Gradient-Boosted Trees\": \n",
    "    \n",
    "    mr.Md(\"## Tree-Based Algorithms\")\n",
    "    mr.Md(\"------\")\n",
    "    mr.Md(\"Similar to $k$-Nearest neighbors, tree-based algorithms are empirically-driven models which determine an \" +\n",
    "         \"outcome based off previous data points in the training set. However, tree-based models wind up being \" +\n",
    "         \"significantly more robust as they do not ultimately depend on the metric structure of the predictor space \" +\n",
    "         \"(though they do depend on the underlying topology, as they are ultimately dependent on decision-trees).\")\n",
    "    \n",
    "    mr.Md(\"<br/>\")\n",
    "    mr.Md(\"### Gradient Boosted Trees\")\n",
    "    mr.Md(\"--------\")\n",
    "    mr.Md(\"Some description of boosted trees here\")\n",
    "\n",
    "    ############################\n",
    "    # Step 1: Construct Pipeline\n",
    "    ############################\n",
    "    pipe_boosted_trees = Pipeline([\n",
    "        ('scaler', preprocessing.StandardScaler()), \n",
    "        ('classifier', GradientBoostingRegressor(random_state=SEED))\n",
    "    ])\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 2: Gather user input\n",
    "    ################################# \n",
    "\n",
    "    features_input = mr.Range(label=f\"How many random {stonks.granularity}in intervals each tree in our forest can see?\",\n",
    "                              value=[int(stonks.lag_length/4), int(stonks.lag_length/2)],\n",
    "                              min=1,\n",
    "                              max=int(stonks.lag_length))\n",
    "\n",
    "    estimators_input = mr.Range(label=f\"How many trees should be in our forest?\",\n",
    "                                value=[10, 20],\n",
    "                                min=1,\n",
    "                                max=150)\n",
    "    learning_input = mr.Range(label=\"Provide an exponent range for N such that 2^N is our learning rate\",\n",
    "                              value=[-8,-1],\n",
    "                              min=-12,\n",
    "                              max=4)\n",
    "\n",
    "    n_features = list(set(np.linspace(features_input.value[0],\n",
    "                                      features_input.value[1],\n",
    "                                      8,\n",
    "                                      dtype=int,\n",
    "                                      endpoint=True) ))\n",
    "    n_estimators = list(set(np.linspace(estimators_input.value[0],\n",
    "                                        estimators_input.value[1],\n",
    "                                        10,\n",
    "                                        dtype=int,\n",
    "                                        endpoint=True) ))\n",
    "    n_learning = [(2**exp) for exp in range(learning_input.value[0], learning_input.value[1] + 1)]\n",
    "\n",
    "\n",
    "    params={'classifier__max_features': n_features,\n",
    "            'classifier__n_estimators': n_estimators,\n",
    "            'classifier__learning_rate':n_learning\n",
    "            }\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 3: Grid Search\n",
    "    ################################# \n",
    "\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "        print(f\"\\n\\nRunning {CROSS_FOLDS * len(n_features) * len(n_learning) * len(n_estimators)} models .....\\n\\n\" )\n",
    "        search = GridSearchCV(pipe_boosted_trees,\n",
    "                              params,\n",
    "                              scoring = PERFORMANCE_METRIC,\n",
    "                              cv=CROSS_FOLDS,\n",
    "                              n_jobs=PARALLELIZATION,\n",
    "                             )\n",
    "        search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # Step 4: Plot Grid Performance\n",
    "    #################################\n",
    "    scores_mean = search.cv_results_['mean_test_score'].reshape(len(n_learning),\n",
    "                                                                len(n_features),\n",
    "                                                                len(n_estimators)\n",
    "                                                               )\n",
    "    # create pandas.DataFrame to pr\n",
    "    boosted_trees_performance = pd.DataFrame({\n",
    "        \"Trees\" : n_estimators\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    titles=[f'n_features={feature}' for feature in n_features]\n",
    "    subs= make_subplots(rows=len(n_features),\n",
    "                        cols=1, \n",
    "                        subplot_titles=titles\n",
    "                       )\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for idx_1, val_1 in enumerate(n_features):\n",
    "            for idx_2, val_2 in enumerate(n_learning):\n",
    "                learning_rate_str = \"learning_rate_\" + str(val_2)  + \"_features_\" + str(val_1)\n",
    "                boosted_trees_performance[learning_rate_str] = scores_mean[idx_2,idx_1,:]\n",
    "\n",
    "            figure_traces = []\n",
    "            fig = px.line(boosted_trees_performance, x=\"Trees\",\n",
    "                          y=[col for col in boosted_trees_performance.columns if f'features_{val_1}' in col],\n",
    "                         title=f\"Random Forest Performance (features={val_1})\").update_layout(yaxis_title=\"R^2 value\")\n",
    "            for trace in range(len(fig[\"data\"])):\n",
    "                figure_traces.append(fig[\"data\"][trace])\n",
    "            for traces in figure_traces:\n",
    "                subs.append_trace(traces, row=(idx_1 + 1), col=1)\n",
    "\n",
    "            subs.update_xaxes(title_text=\"# Trees\", row=(idx_1 + 1), col=1)\n",
    "\n",
    "\n",
    "        subs.update_layout(height=(220 * len(n_estimators)), title_text=\"Boosted Trees Performance\")\n",
    "        subs.show()        \n",
    "\n",
    "\n",
    "    mr.Md(f\"\\nThe best performing Boosted Trees model had {search.best_params_['classifier__n_estimators']} trees;\" +\n",
    "         f\"\\neach tree influenced the construction of the next tree with a learning rate of {search.best_params_['classifier__learning_rate']}.\" +\n",
    "         f\"\\nEach decision tree only considered {search.best_params_['classifier__max_features']}\" +\n",
    "         f\" {stonks.granularity}in intervals when looking for the best split.\\nThis gave a mean R^2 score of \" +\n",
    "         f\"{search.best_score_} across the {CROSS_FOLDS} folds of the training data.\\n\")    \n",
    "\n",
    "\n",
    "    mr.Md(\"\\n\\n\")\n",
    "    mr.Md('Training set score: ' + str(search.score(X_train, Y_train)))\n",
    "    mr.Md('Test set score: ' + str(search.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e348a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a105bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
